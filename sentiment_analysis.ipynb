{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SONAR: Emotionally Intelligent Music Recommendations\n",
    "\n",
    "Draws heavily from [this tutorial](https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html)\n",
    "\n",
    "Welcome to Sonar! This project was inpired me to first explore Machine Learning. The premise is simple: we often want different music to listen to based on our mood. However, oftentimes finding that music proves difficult. Sonar takes away that difficulty: by entering short mood messages, sonar generates Spotify recommendations using both your old favorites and determined mood. \n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import bokeh\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various document-wide utilities set up below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182ef1ebce214c248227fa95a548c1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "tqdm_notebook().pandas(desc=\"progress-bar\")\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "TaggedDocument = gensim.models.doc2vec.TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Load tweets into a usable format. Binarize sentiment from {0, 1, 2, 3, 4} to {0, 1} with 2 serving as the cut off for a \"positive\" sentiment. Clean up null values and reindex.\n",
    "\n",
    "- Tokenize the data and take only the n values we aim to use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets():\n",
    "    tweets_train = pd.read_csv(\"/home/malits/data/twitter_sentiment_analysis/training.1600000.processed.noemoticon.csv\", \n",
    "                               encoding = \"ISO-8859-1\")\n",
    "    data = pd.DataFrame()\n",
    "    data[\"sentiment\"] = tweets_train[\"0\"].apply(lambda v: 1 if (v >= 2) else 0)\n",
    "    data[\"sentiment\"] = data[\"sentiment\"].map(int)\n",
    "    data[\"tweet\"] = tweets_train.iloc[:, [5]]\n",
    "    data[\"tweet\"] = data[\"tweet\"].astype(str)\n",
    "    data = data[data['tweet'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0          0  is upset that he can't update his Facebook by ...\n",
       "1          0  @Kenichan I dived many times for the ball. Man...\n",
       "2          0    my whole body feels itchy and like its on fire \n",
       "3          0  @nationwideclass no, it's not behaving at all....\n",
       "4          0                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_tweets()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    sub_regex = r'[(#[A-Za-z0-9]+)(https.+)\\?!\\.,]'\n",
    "    tweet = str(tweet)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tokens = [re.sub(sub_regex, '', t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t is not None and t != '']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data[\"tokens\"] = data[\"tweet\"].progress_apply(tokenize)\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f4a99fa5604910a61ed6b34f54f5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='progress-bar', max=500, style=ProgressStyle(description_widthâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malits/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "processed = postprocess(data.head(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once preprocessed, split to test/train sets and vectorize w w2vec for use in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of our embedding space\n",
    "\n",
    "N_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(processed['tokens']),\n",
    "                                                    np.array(processed['sentiment']),\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i, v in tqdm_notebook(enumerate(tweets)):\n",
    "        label = '{}_{}'.format(label_type, i)\n",
    "        ## Use nltk TaggedDocument obj to mark words with their corresponding label. Makes words easier to retrieve from their vector form.\n",
    "        labelized.append(TaggedDocument(v, [label]))\n",
    "    return labelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168015bf125d41abbf5dd1bafec3f831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d714832472f44dd9f6fa0e1cc158806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = labelizeTweets(X_train, 'TRAIN')\n",
    "X_test = labelizeTweets(X_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c473ddea4e954c71ae2100ba3b725161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c192e5247e1645af9a0120a42f042c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 08:29:00.642506 139739017860928 base_any2vec.py:686] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4686, 27600)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(X_train)\n",
    "tweet_w2v = Word2Vec(size=N_DIM, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm_notebook(X_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm_notebook(X_train)], \n",
    "                total_examples=train_size, epochs = tweet_w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malits/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('of', 0.3715263605117798),\n",
       " ('I', 0.3577388525009155),\n",
       " ('the', 0.34806740283966064),\n",
       " ('that', 0.34661033749580383),\n",
       " ('\"', 0.3401591181755066),\n",
       " ('got', 0.3271946907043457),\n",
       " ('just', 0.3248497545719147),\n",
       " ('my', 0.322878360748291),\n",
       " (\"I'm\", 0.3183445930480957),\n",
       " ('to', 0.3160049319267273)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
