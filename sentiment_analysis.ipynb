{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SONAR: Emotionally Intelligent Music Recommendations\n",
    "\n",
    "Draws heavily from [this tutorial](https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html)\n",
    "\n",
    "Welcome to Sonar! This project was inpired me to first explore Machine Learning. The premise is simple: we often want different music to listen to based on our mood. However, oftentimes finding that music proves difficult. Sonar takes away that difficulty: by entering short mood messages, sonar generates Spotify recommendations using both your old favorites and determined mood. \n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f245a40992d4308ac0a31bd73c09e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import spacy\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "tqdm_notebook().pandas(desc=\"progress-bar\")\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "TaggedDocument = gensim.models.doc2vec.TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Load tweets into a usable format. Binarize sentiment from {0, 1, 2, 3, 4} to {0, 1} with 2 serving as the cut off for a \"positive\" sentiment. Clean up null values and reindex.\n",
    "\n",
    "- Tokenize the data and take only the n values we aim to use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets():\n",
    "    tweets_train = pd.read_csv(\"/home/malits/data/twitter_sentiment_analysis/training.1600000.processed.noemoticon.csv\", \n",
    "                               encoding = \"ISO-8859-1\")\n",
    "    data = pd.DataFrame()\n",
    "    data[\"sentiment\"] = tweets_train[\"0\"].apply(lambda v: 1 if (v >= 2) else 0)\n",
    "    data[\"sentiment\"] = data[\"sentiment\"].map(int)\n",
    "    data[\"tweet\"] = tweets_train.iloc[:, [5]]\n",
    "    data[\"tweet\"] = data[\"tweet\"].astype(str)\n",
    "    data = data[data['tweet'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0          0  is upset that he can't update his Facebook by ...\n",
       "1          0  @Kenichan I dived many times for the ball. Man...\n",
       "2          0    my whole body feels itchy and like its on fire \n",
       "3          0  @nationwideclass no, it's not behaving at all....\n",
       "4          0                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_tweets()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    sub_regex = r'(#[A-Za-z0-9])|(https)\\?!\\.,'\n",
    "    tweet = str(tweet)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tokens = [re.sub(sub_regex, '', t) for t in tokens]\n",
    "    tokens = [t.lower() for t in tokens if t is not None and t != '']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data.loc[:, \"tokens\"] = data.loc[:, \"tweet\"].progress_apply(tokenize)\n",
    "    data = data.loc[data.tokens != 'NC', :]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3fe46a13d3406dae98edb934db0f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='progress-bar', max=1000000, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malits/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/malits/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "processed = postprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once preprocessed, split to test/train sets and vectorize w w2vec for use in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of our embedding space\n",
    "\n",
    "N_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(processed['tokens']),\n",
    "                                                    np.array(processed['sentiment']),\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i, v in tqdm_notebook(enumerate(tweets)):\n",
    "        label = '{}_{}'.format(label_type, i)\n",
    "        ## Use nltk TaggedDocument obj to mark words with their corresponding label. Makes words easier to retrieve from their vector form.\n",
    "        labelized.append(TaggedDocument(v, [label]))\n",
    "    return labelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f5f2a3005e46a7bc0a6fe905972619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55f79f7fa7349aaa41a3a5d0e984d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = labelizeTweets(X_train, 'TRAIN')\n",
    "X_test = labelizeTweets(X_test, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Word2Vec corpus with more data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e81109d57a243459cca1ee100bf431a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7bed8d232c440fb062b862450cc093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43227464, 60104860)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(X_train)\n",
    "tweet_w2v = Word2Vec(size=N_DIM, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm_notebook(X_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm_notebook(X_train)], \n",
    "                total_examples=train_size, epochs = tweet_w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(text):\n",
    "    return tweet_w2v.wv.most_similar(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('echo', 0.6594349145889282),\n",
       " ('indies', 0.6447641253471375),\n",
       " ('alexander', 0.6290473937988281),\n",
       " ('undead', 0.6146681904792786),\n",
       " ('cattle', 0.6079335808753967),\n",
       " ('shore', 0.6066163182258606),\n",
       " ('collective', 0.602022647857666),\n",
       " ('brom', 0.5925573110580444),\n",
       " ('ford', 0.5919807553291321),\n",
       " ('african', 0.5899761915206909)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('kanye')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use TF-IDF weightings to weight word vectors accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweet_vector(words, ndims):\n",
    "    vector = np.zeros((1, ndims))\n",
    "    count = 0\n",
    "    \n",
    "    for w in words:\n",
    "        try:\n",
    "            vector += tweet_w2v.wv[w].reshape((1, ndims)) * tfidf[w]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    return np.divide(vector,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1984d9a4b849f298966a6d2ee98ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malits/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1b0b706b264633a36d836f15228a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malits/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "train_vec_tweets = np.concatenate([create_tweet_vector(x, N_DIM) for x in tqdm_notebook(map(lambda x: x.words, X_train))])\n",
    "train_vec_tweets = scale(train_vec_tweets)\n",
    "\n",
    "test_vec_tweets = np.concatenate([create_tweet_vector(x, N_DIM) for x in tqdm_notebook(map(lambda x: x.words, X_test))])\n",
    "test_vec_tweets = scale(test_vec_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Architecture in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "800000/800000 [==============================] - 29s 36us/step - loss: nan - acc: 1.7750e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd442cef8d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vec_tweets, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 3s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, 0.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_vec_tweets, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10839082,  0.96090095,  1.19466249, ...,  0.04442698,\n",
       "        -2.70746003, -0.36207966],\n",
       "       [ 0.49944944,  0.6660377 ,  1.02380939, ..., -0.57288695,\n",
       "        -0.27184162, -0.02124841],\n",
       "       [ 0.25561972, -0.33809096,  0.32096454, ..., -0.57700449,\n",
       "         0.89326356,  0.82663036],\n",
       "       ...,\n",
       "       [ 0.1258103 ,  0.53280072,  0.28238178, ...,  0.70347905,\n",
       "        -0.40420087,  0.21112377],\n",
       "       [ 1.8908536 ,  1.22318712, -1.38479328, ...,  0.73263885,\n",
       "        -0.0792803 , -1.56691066],\n",
       "       [-0.90170334,  0.2833283 ,  1.54325774, ..., -3.53057298,\n",
       "         3.5072139 , -1.53493829]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
