{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, LSTM\n",
    "from keras.models import load_model, save_model\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Flatten, SpatialDropout1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.casual import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('/home/malits/data/emotion/train_data.csv',\n",
    "                           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'sadness',\n",
       " 'enthusiasm',\n",
       " 'empty',\n",
       " 'hate',\n",
       " 'surprise',\n",
       " 'boredom',\n",
       " 'anger',\n",
       " 'happiness',\n",
       " 'neutral',\n",
       " 'fun',\n",
       " 'worry',\n",
       " 'relief']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(training_data.sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "new_stops = set(stop_words)\n",
    "new_stops.remove(\"not\")\n",
    "\n",
    "for s in stop_words:\n",
    "    new_stops.add(s.replace('\\'', ''))\n",
    "    \n",
    "filters = '!\"#$%&()*+,-./:;<=>?[\\]^_`{|}~\\t\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    clean_tokens = []\n",
    "    \n",
    "    for text in texts:\n",
    "        seq = text_to_word_sequence(text,filters=filters,lower=True)\n",
    "        \n",
    "        text = str(text)\n",
    "        text = text.replace('\\'', '')\n",
    "        text = text.lower()\n",
    "        toks = [t for t in seq if not t.startswith(\"@\")]\n",
    "        toks = [tok for tok in toks if tok not in new_stops]\n",
    "        clean_tokens.append(toks)\n",
    "        \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"processed_content\"] = preprocess(training_data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized to Word indices as (30000,)\n"
     ]
    }
   ],
   "source": [
    "word_sequences = training_data.processed_content\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(word_sequences)\n",
    "word_indices = tokenizer.texts_to_sequences(word_sequences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Tokenized to Word indices as {np.array(word_indices).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Word Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = pad_sequences(word_indices, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer using GloVe 50D pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_layer(dim=EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join('/home/malits/data/glove/', 'glove.6B.50d.txt'),\n",
    "                            'r', encoding='utf-8')\n",
    "    # Open and parse GloVe file\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Loaded GloVe Vectors')\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, dim))\n",
    "    for word, i in word_index.items():\n",
    "        # populate embedding matrix with GloVe vectors\n",
    "        # leave unkown words to be all zeros\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    print(f'Emedding Matrix Generated with Shape {embedding_matrix.shape}')\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1,dim, \n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    \n",
    "    return embedding_layer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_encodings(data):\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(data)\n",
    "    le_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),\n",
    "                               label_encoder.classes_))\n",
    "    print(f\"Label Encoding Classes as {le_name_mapping}\")\n",
    "    \n",
    "    binarized_data = np_utils.to_categorical(integer_encoded)\n",
    "    print(f\"One Hot Encoded class shape {binarized_data.shape}\")\n",
    "    \n",
    "    return binarized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe Vectors\n",
      "Emedding Matrix Generated with Shape (27968, 50)\n",
      "Label Encoding Classes as {0: 'anger', 1: 'boredom', 2: 'empty', 3: 'enthusiasm', 4: 'fun', 5: 'happiness', 6: 'hate', 7: 'love', 8: 'neutral', 9: 'relief', 10: 'sadness', 11: 'surprise', 12: 'worry'}\n",
      "One Hot Encoded class shape (30000, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 11:40:54.514535 140563970615104 deprecation.py:506] From /home/malits/anaconda3/envs/sonar/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embedding = make_embedding_layer()\n",
    "binary_encodings = make_binary_encodings(training_data.sentiment)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(binary_encodings.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_data, \n",
    "                                                    binary_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22500/22500 [==============================] - 17s 756us/step - loss: 2.0826 - acc: 0.2761\n",
      "Epoch 2/100\n",
      "  128/22500 [..............................] - ETA: 21s - loss: 2.0475 - acc: 0.3359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malits/anaconda3/envs/sonar/lib/python3.7/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500/22500 [==============================] - 15s 685us/step - loss: 2.0130 - acc: 0.3011\n",
      "Epoch 3/100\n",
      "22500/22500 [==============================] - 17s 737us/step - loss: 1.9884 - acc: 0.3095\n",
      "Epoch 4/100\n",
      "22500/22500 [==============================] - 15s 684us/step - loss: 1.9685 - acc: 0.3128\n",
      "Epoch 5/100\n",
      "22500/22500 [==============================] - 17s 748us/step - loss: 1.9546 - acc: 0.3231\n",
      "Epoch 6/100\n",
      "22500/22500 [==============================] - 17s 762us/step - loss: 1.9391 - acc: 0.3243\n",
      "Epoch 7/100\n",
      "22500/22500 [==============================] - 15s 688us/step - loss: 1.9305 - acc: 0.3242\n",
      "Epoch 8/100\n",
      "22500/22500 [==============================] - 15s 669us/step - loss: 1.9197 - acc: 0.3304\n",
      "Epoch 9/100\n",
      "22500/22500 [==============================] - 15s 681us/step - loss: 1.9106 - acc: 0.3340\n",
      "Epoch 10/100\n",
      "22500/22500 [==============================] - 15s 656us/step - loss: 1.9055 - acc: 0.3379\n",
      "Epoch 11/100\n",
      "22500/22500 [==============================] - 15s 657us/step - loss: 1.8956 - acc: 0.3364\n",
      "Epoch 12/100\n",
      "22500/22500 [==============================] - 15s 656us/step - loss: 1.8885 - acc: 0.3428\n",
      "Epoch 13/100\n",
      "22500/22500 [==============================] - 15s 659us/step - loss: 1.8846 - acc: 0.3462\n",
      "Epoch 14/100\n",
      "22500/22500 [==============================] - 15s 666us/step - loss: 1.8778 - acc: 0.3478\n",
      "Epoch 15/100\n",
      "22500/22500 [==============================] - 15s 662us/step - loss: 1.8689 - acc: 0.3491\n",
      "Epoch 16/100\n",
      "22500/22500 [==============================] - 15s 670us/step - loss: 1.8624 - acc: 0.3486\n",
      "Epoch 17/100\n",
      "22500/22500 [==============================] - 15s 657us/step - loss: 1.8551 - acc: 0.3569\n",
      "Epoch 18/100\n",
      "22500/22500 [==============================] - 15s 660us/step - loss: 1.8494 - acc: 0.3560\n",
      "Epoch 19/100\n",
      "22500/22500 [==============================] - 15s 671us/step - loss: 1.8444 - acc: 0.3560\n",
      "Epoch 20/100\n",
      "22500/22500 [==============================] - 15s 681us/step - loss: 1.8406 - acc: 0.3576\n",
      "Epoch 21/100\n",
      "22500/22500 [==============================] - 15s 663us/step - loss: 1.8348 - acc: 0.3640\n",
      "Epoch 22/100\n",
      "22500/22500 [==============================] - 15s 663us/step - loss: 1.8306 - acc: 0.3649\n",
      "Epoch 23/100\n",
      "22500/22500 [==============================] - 15s 660us/step - loss: 1.8216 - acc: 0.3675\n",
      "Epoch 24/100\n",
      "22500/22500 [==============================] - 15s 666us/step - loss: 1.8151 - acc: 0.3676\n",
      "Epoch 25/100\n",
      "22500/22500 [==============================] - 16s 714us/step - loss: 1.8149 - acc: 0.3697\n",
      "Epoch 26/100\n",
      "22500/22500 [==============================] - 16s 704us/step - loss: 1.8084 - acc: 0.3716\n",
      "Epoch 27/100\n",
      "22500/22500 [==============================] - 16s 694us/step - loss: 1.8050 - acc: 0.3748\n",
      "Epoch 28/100\n",
      "22500/22500 [==============================] - 15s 658us/step - loss: 1.7991 - acc: 0.3743\n",
      "Epoch 29/100\n",
      "22500/22500 [==============================] - 15s 685us/step - loss: 1.7926 - acc: 0.3736\n",
      "Epoch 30/100\n",
      "22500/22500 [==============================] - 16s 697us/step - loss: 1.7889 - acc: 0.3746\n",
      "Epoch 31/100\n",
      "22500/22500 [==============================] - 15s 688us/step - loss: 1.7877 - acc: 0.3751\n",
      "Epoch 32/100\n",
      "22500/22500 [==============================] - 15s 689us/step - loss: 1.7861 - acc: 0.3796\n",
      "Epoch 33/100\n",
      "22500/22500 [==============================] - 15s 667us/step - loss: 1.7753 - acc: 0.3831\n",
      "Epoch 34/100\n",
      "22500/22500 [==============================] - 15s 666us/step - loss: 1.7783 - acc: 0.3765\n",
      "Epoch 35/100\n",
      "22500/22500 [==============================] - 15s 668us/step - loss: 1.7763 - acc: 0.3792\n",
      "Epoch 36/100\n",
      "22500/22500 [==============================] - 15s 668us/step - loss: 1.7676 - acc: 0.3851\n",
      "Epoch 37/100\n",
      "22500/22500 [==============================] - 15s 673us/step - loss: 1.7613 - acc: 0.3877\n",
      "Epoch 38/100\n",
      "22500/22500 [==============================] - 15s 667us/step - loss: 1.7585 - acc: 0.3876\n",
      "Epoch 39/100\n",
      "22500/22500 [==============================] - 15s 671us/step - loss: 1.7638 - acc: 0.3823\n",
      "Epoch 40/100\n",
      "22500/22500 [==============================] - 15s 672us/step - loss: 1.7508 - acc: 0.3884\n",
      "Epoch 41/100\n",
      "22500/22500 [==============================] - 15s 673us/step - loss: 1.7539 - acc: 0.3896\n",
      "Epoch 42/100\n",
      "22500/22500 [==============================] - 15s 674us/step - loss: 1.7471 - acc: 0.3908\n",
      "Epoch 43/100\n",
      "22500/22500 [==============================] - 15s 686us/step - loss: 1.7494 - acc: 0.3897\n",
      "Epoch 44/100\n",
      "22500/22500 [==============================] - 16s 727us/step - loss: 1.7476 - acc: 0.3916\n",
      "Epoch 45/100\n",
      "22500/22500 [==============================] - 18s 801us/step - loss: 1.7423 - acc: 0.3925\n",
      "Epoch 46/100\n",
      "22500/22500 [==============================] - 17s 774us/step - loss: 1.7382 - acc: 0.3906\n",
      "Epoch 47/100\n",
      "22500/22500 [==============================] - 15s 679us/step - loss: 1.7385 - acc: 0.3926\n",
      "Epoch 48/100\n",
      "22500/22500 [==============================] - 16s 704us/step - loss: 1.7316 - acc: 0.3956\n",
      "Epoch 49/100\n",
      "22500/22500 [==============================] - 18s 798us/step - loss: 1.7298 - acc: 0.3955\n",
      "Epoch 50/100\n",
      "22500/22500 [==============================] - 16s 720us/step - loss: 1.7301 - acc: 0.3953\n",
      "Epoch 51/100\n",
      "22500/22500 [==============================] - 17s 776us/step - loss: 1.7285 - acc: 0.3999\n",
      "Epoch 52/100\n",
      "22500/22500 [==============================] - 19s 837us/step - loss: 1.7258 - acc: 0.3976\n",
      "Epoch 53/100\n",
      "22500/22500 [==============================] - 16s 724us/step - loss: 1.7197 - acc: 0.3994\n",
      "Epoch 54/100\n",
      "22500/22500 [==============================] - 17s 757us/step - loss: 1.7210 - acc: 0.3982\n",
      "Epoch 55/100\n",
      "22500/22500 [==============================] - 18s 819us/step - loss: 1.7195 - acc: 0.4012\n",
      "Epoch 56/100\n",
      "22500/22500 [==============================] - 17s 739us/step - loss: 1.7205 - acc: 0.3979\n",
      "Epoch 57/100\n",
      "22500/22500 [==============================] - 16s 696us/step - loss: 1.7158 - acc: 0.4001\n",
      "Epoch 58/100\n",
      "22500/22500 [==============================] - 16s 697us/step - loss: 1.7110 - acc: 0.4019\n",
      "Epoch 59/100\n",
      "22500/22500 [==============================] - 16s 722us/step - loss: 1.7090 - acc: 0.4028\n",
      "Epoch 60/100\n",
      "22500/22500 [==============================] - 17s 746us/step - loss: 1.7102 - acc: 0.4012\n",
      "Epoch 61/100\n",
      "22500/22500 [==============================] - 16s 715us/step - loss: 1.7045 - acc: 0.4057\n",
      "Epoch 62/100\n",
      "22500/22500 [==============================] - 17s 765us/step - loss: 1.7049 - acc: 0.4036\n",
      "Epoch 63/100\n",
      "22500/22500 [==============================] - 16s 715us/step - loss: 1.7029 - acc: 0.4032\n",
      "Epoch 64/100\n",
      "22500/22500 [==============================] - 16s 691us/step - loss: 1.7024 - acc: 0.4063\n",
      "Epoch 65/100\n",
      "22500/22500 [==============================] - 16s 693us/step - loss: 1.6973 - acc: 0.4070\n",
      "Epoch 66/100\n",
      "22500/22500 [==============================] - 16s 699us/step - loss: 1.6968 - acc: 0.4074\n",
      "Epoch 67/100\n",
      "22500/22500 [==============================] - 16s 705us/step - loss: 1.6962 - acc: 0.4072\n",
      "Epoch 68/100\n",
      "22500/22500 [==============================] - 16s 712us/step - loss: 1.6935 - acc: 0.4064\n",
      "Epoch 69/100\n",
      "22500/22500 [==============================] - 16s 723us/step - loss: 1.6868 - acc: 0.4069\n",
      "Epoch 70/100\n",
      "22500/22500 [==============================] - 16s 715us/step - loss: 1.6866 - acc: 0.4112\n",
      "Epoch 71/100\n",
      "22500/22500 [==============================] - 20s 872us/step - loss: 1.6851 - acc: 0.4101\n",
      "Epoch 72/100\n",
      "22500/22500 [==============================] - 18s 781us/step - loss: 1.6875 - acc: 0.4085\n",
      "Epoch 73/100\n",
      "22500/22500 [==============================] - 18s 781us/step - loss: 1.6858 - acc: 0.4105\n",
      "Epoch 74/100\n",
      "22500/22500 [==============================] - 16s 729us/step - loss: 1.6851 - acc: 0.4093\n",
      "Epoch 75/100\n",
      "22500/22500 [==============================] - 16s 733us/step - loss: 1.6807 - acc: 0.4099\n",
      "Epoch 76/100\n",
      "22500/22500 [==============================] - 18s 813us/step - loss: 1.6784 - acc: 0.4117\n",
      "Epoch 77/100\n",
      "22500/22500 [==============================] - 17s 757us/step - loss: 1.6812 - acc: 0.4128\n",
      "Epoch 78/100\n",
      "22500/22500 [==============================] - 19s 835us/step - loss: 1.6747 - acc: 0.4153\n",
      "Epoch 79/100\n",
      "22500/22500 [==============================] - 18s 817us/step - loss: 1.6745 - acc: 0.4131\n",
      "Epoch 80/100\n",
      "22500/22500 [==============================] - 16s 710us/step - loss: 1.6847 - acc: 0.4087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "22500/22500 [==============================] - 15s 648us/step - loss: 1.6753 - acc: 0.4135\n",
      "Epoch 82/100\n",
      "22500/22500 [==============================] - 16s 712us/step - loss: 1.6685 - acc: 0.4158\n",
      "Epoch 83/100\n",
      "22500/22500 [==============================] - 15s 646us/step - loss: 1.6828 - acc: 0.4050\n",
      "Epoch 84/100\n",
      "22500/22500 [==============================] - 15s 652us/step - loss: 1.6668 - acc: 0.4177\n",
      "Epoch 85/100\n",
      "22500/22500 [==============================] - 15s 674us/step - loss: 1.6694 - acc: 0.4178\n",
      "Epoch 86/100\n",
      "22500/22500 [==============================] - 15s 661us/step - loss: 1.6717 - acc: 0.4161\n",
      "Epoch 87/100\n",
      "22500/22500 [==============================] - 15s 647us/step - loss: 1.6661 - acc: 0.4151\n",
      "Epoch 88/100\n",
      "22500/22500 [==============================] - 16s 709us/step - loss: 1.6732 - acc: 0.4180\n",
      "Epoch 89/100\n",
      "22500/22500 [==============================] - 16s 728us/step - loss: 1.6670 - acc: 0.4173\n",
      "Epoch 90/100\n",
      "22500/22500 [==============================] - 18s 795us/step - loss: 1.6707 - acc: 0.4120\n",
      "Epoch 91/100\n",
      "22500/22500 [==============================] - 19s 836us/step - loss: 1.6650 - acc: 0.4178\n",
      "Epoch 92/100\n",
      "22500/22500 [==============================] - 21s 926us/step - loss: 1.6609 - acc: 0.4210\n",
      "Epoch 93/100\n",
      "22500/22500 [==============================] - 21s 924us/step - loss: 1.6637 - acc: 0.4151\n",
      "Epoch 94/100\n",
      "22500/22500 [==============================] - 19s 866us/step - loss: 1.6604 - acc: 0.4185\n",
      "Epoch 95/100\n",
      "22500/22500 [==============================] - 19s 865us/step - loss: 1.6601 - acc: 0.4212\n",
      "Epoch 96/100\n",
      "22500/22500 [==============================] - 19s 863us/step - loss: 1.6689 - acc: 0.4132\n",
      "Epoch 97/100\n",
      "22500/22500 [==============================] - 19s 824us/step - loss: 1.6533 - acc: 0.4207\n",
      "Epoch 98/100\n",
      "22500/22500 [==============================] - 20s 897us/step - loss: 1.6628 - acc: 0.4178\n",
      "Epoch 99/100\n",
      "22500/22500 [==============================] - 18s 808us/step - loss: 1.6538 - acc: 0.4214\n",
      "Epoch 100/100\n",
      "22500/22500 [==============================] - 18s 782us/step - loss: 1.6528 - acc: 0.4222\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=num_epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                                            patience=3,\n",
    "                                            min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
